{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Cross Validation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Evaluates the best classifier and the best parameters for application.\n",
    "\n",
    "<b>Decision Tree:</b> 98,30%<br>\n",
    "<b>Random Forest:</b> 98,70%<br>\n",
    "<b>KNN:</b> 98,00%<br>\n",
    "<b>Logistic Regression:</b> 94,85%<br>\n",
    "<b>SVM:</b> 98,30%<br>\n",
    "<b>Neural Network:</b> 99,64%<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Base Credit Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Import Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "# necessary step to standardize categories\n",
    "with open('../examples/credit.pkl', 'rb') as f:\n",
    "  x_credit_trainning, y_credit_trainning, x_credit_test, y_credit_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_credit_trainning.shape, y_credit_trainning.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_credit_test.shape, y_credit_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concatenate x credit trainning and test in the same array\n",
    "x_credit = np.concatenate((x_credit_trainning, x_credit_test), axis = 0)\n",
    "x_credit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concatenate y credit trainning and test in the same array\n",
    "y_credit = np.concatenate((y_credit_trainning, y_credit_test), axis = 0)\n",
    "y_credit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_credit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Get best learning params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree - 98,3%\n",
    "params = {'criterion': ['gini', 'entropy'],\n",
    "          'splitter': ['best', 'random'],\n",
    "          'min_samples_split': [2, 5, 10],\n",
    "          'min_samples_leaf': [1, 5, 10]}\n",
    "grid_search = GridSearchCV(estimator=DecisionTreeClassifier(), param_grid=params)\n",
    "grid_search.fit(x_credit, y_credit)\n",
    "best_params = grid_search.best_params_\n",
    "print(best_params) # {'criterion': 'entropy', 'min_samples_leaf': 1, 'min_samples_split': 5, 'splitter': 'best'}\n",
    "best_result = grid_search.best_score_\n",
    "print(best_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest - 98,7%\n",
    "params = {'criterion': ['gini', 'entropy'],\n",
    "          'n_estimators': [10, 40, 100, 150],\n",
    "          'min_samples_split': [2, 5, 10],\n",
    "          'min_samples_leaf': [1, 5, 10]}\n",
    "grid_search = GridSearchCV(estimator=RandomForestClassifier(), param_grid=params)\n",
    "grid_search.fit(x_credit, y_credit)\n",
    "best_params = grid_search.best_params_\n",
    "print(best_params) # {'criterion': 'entropy', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 150}\n",
    "best_result = grid_search.best_score_\n",
    "print(best_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN (Instances) - 98,0%\n",
    "params = {'n_neighbors': [3, 5, 10, 20],\n",
    "          'p': [1, 2]}\n",
    "grid_search = GridSearchCV(estimator=KNeighborsClassifier(), param_grid=params)\n",
    "grid_search.fit(x_credit, y_credit)\n",
    "best_params = grid_search.best_params_\n",
    "print(best_params) # {'n_neighbors': 20, 'p': 1}\n",
    "best_result = grid_search.best_score_\n",
    "print(best_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression - 94,85%\n",
    "params = {'tol': [0.0001, 0.00001, 0.000001],\n",
    "          'C': [1.0, 1.5, 2.0],\n",
    "          'solver': ['lbfgs', 'sag', 'saga']}\n",
    "grid_search = GridSearchCV(estimator=LogisticRegression(), param_grid=params)\n",
    "grid_search.fit(x_credit, y_credit)\n",
    "best_params = grid_search.best_params_\n",
    "print(best_params) # {'C': 1.0, 'solver': 'lbfgs', 'tol': 0.0001}\n",
    "best_result = grid_search.best_score_\n",
    "print(best_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM - 98,30%\n",
    "params = {'tol': [0.001, 0.0001, 0.00001],\n",
    "          'C': [1.0, 1.5, 2.0],\n",
    "          'kernel': ['rbf', 'linear', 'poly', 'sigmoid']}\n",
    "grid_search = GridSearchCV(estimator=SVC(), param_grid=params)\n",
    "grid_search.fit(x_credit, y_credit)\n",
    "best_params = grid_search.best_params_\n",
    "print(best_params) # {'C': 1.5, 'kernel': 'rbf', 'tol': 0.001}\n",
    "best_result = grid_search.best_score_\n",
    "print(best_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network - 99,64%\n",
    "params = {'activation': ['relu', 'logistic', 'tahn'],\n",
    "          'solver': ['adam', 'sgd'],\n",
    "          'batch_size': [10, 56]}\n",
    "grid_search = GridSearchCV(estimator=MLPClassifier(), param_grid=params)\n",
    "grid_search.fit(x_credit, y_credit)\n",
    "best_params = grid_search.best_params_\n",
    "print(best_params) # {'activation': 'relu', 'batch_size': 10, 'solver': 'adam'}\n",
    "best_result = grid_search.best_score_\n",
    "print(best_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Get score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "result_decision_tree = []\n",
    "result_random_forest = []\n",
    "result_knn = []\n",
    "result_logistic_regression = []\n",
    "result_svm = []\n",
    "result_neural = []\n",
    "\n",
    "# get mean of results of respective algorithm to evaluate\n",
    "for i in range(3):\n",
    "  kfold = KFold(n_splits=10, shuffle=True, random_state=i)\n",
    "  # Decision Tree\n",
    "  tree = DecisionTreeClassifier(criterion='entropy', min_samples_leaf=1, min_samples_split=5, splitter='best')\n",
    "  scores = cross_val_score(tree, x_credit, y_credit, cv=kfold)\n",
    "  result_decision_tree.append(scores.mean())\n",
    "  # Random Forest\n",
    "  forest = RandomForestClassifier(criterion='entropy', min_samples_leaf=1, min_samples_split=5, n_estimators=150)\n",
    "  scores = cross_val_score(forest, x_credit, y_credit, cv=kfold)\n",
    "  result_random_forest.append(scores.mean())\n",
    "  # KNN\n",
    "  knn = KNeighborsClassifier()\n",
    "  scores = cross_val_score(knn, x_credit, y_credit, cv=kfold)\n",
    "  result_knn.append(scores.mean())\n",
    "  # Logistic Regression\n",
    "  regression = LogisticRegression(C=1.0, solver='lbfgs', tol=0.0001)\n",
    "  scores = cross_val_score(regression, x_credit, y_credit, cv=kfold)\n",
    "  result_logistic_regression.append(scores.mean())\n",
    "  # SVM\n",
    "  svm = SVC(C=1.5, kernel='rbf', tol= 0.001)\n",
    "  scores = cross_val_score(svm, x_credit, y_credit, cv=kfold)\n",
    "  result_svm.append(scores.mean())\n",
    "  # Neural Network\n",
    "  neural = MLPClassifier(activation='relu', batch_size=10, solver='adam')\n",
    "  scores = cross_val_score(neural, x_credit, y_credit, cv=kfold)\n",
    "  result_neural.append(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# unify results\n",
    "results = pd.DataFrame({\n",
    "  'Tree': result_decision_tree,\n",
    "  'Forest': result_random_forest,\n",
    "  'Knn': result_knn,\n",
    "  'Regression': result_logistic_regression,\n",
    "  'SVM': result_svm,\n",
    "  'Neural': result_neural})\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe\n",
    "results.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variance\n",
    "results.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coefficient of variation\n",
    "(results.std() / results.mean()) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Check distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checks that the data set comes from a normal distribution\n",
    "from scipy.stats import shapiro\n",
    "# pvalue less than alpha value (0.05 default) means that it is a non-normal distribution\n",
    "shapiro(result_decision_tree), shapiro(result_random_forest), shapiro(result_knn), shapiro(result_logistic_regression), shapiro(result_svm), shapiro(result_neural)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(result_decision_tree, kind='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(result_random_forest, kind='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(result_knn, kind='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(result_logistic_regression, kind='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(result_svm, kind='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(result_neural, kind='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Classificate hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that exists statistics diferences between group of results\n",
    "from scipy.stats import f_oneway\n",
    "_, p = f_oneway(result_decision_tree, result_random_forest, result_knn, result_logistic_regression, result_svm, result_neural)\n",
    "\n",
    "alpha = 0.05\n",
    "if p <= alpha:\n",
    "  print('Null hypothesis rejected, data is differents.')\n",
    "else:\n",
    "  print('Alternative hypothesis rejected, data is equals.')\n",
    "\n",
    "results_algorithm = {'accuracy': np.concatenate([result_decision_tree, result_random_forest, result_knn, result_logistic_regression, result_svm, result_neural]),\n",
    "                    'algorithm': [\n",
    "                      'Tree', 'Tree', 'Tree',\n",
    "                      'Forest', 'Forest', 'Forest',\n",
    "                      'Knn', 'Knn', 'Knn',\n",
    "                      'Regression', 'Regression', 'Regression',\n",
    "                      'SVM', 'SVM', 'SVM',\n",
    "                      'Neural', 'Neural', 'Neural',\n",
    "                    ]}\n",
    "results_df = pd.DataFrame(results_algorithm)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.multicomp import MultiComparison\n",
    "compare_algorithms = MultiComparison(results_df['accuracy'], results_df['algorithm'])\n",
    "# registers not rejected means that not has statistics differences by groups\n",
    "statistics_test = compare_algorithms.tukeyhsd()\n",
    "print(statistics_test)\n",
    "# otherwise, regs rejected must be compare performance by results.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the most advanced category means it has the best performance\n",
    "statistics_test.plot_simultaneous()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
